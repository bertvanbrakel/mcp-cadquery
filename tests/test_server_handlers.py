import pytest
import os
import sys
import uuid
import shutil
import json
import asyncio
import time # Added for sleep in SVG test
from fastapi.testclient import TestClient

# Add back sys.path modification
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))

# Import the app instance, state, and paths from server
# This should work now as they are defined globally after the venv check
from server import (
    app,
    shape_results,
    RENDER_DIR_PATH,
    PART_PREVIEW_DIR_PATH, # Needed for fixture cleanup
    RENDER_DIR_NAME # Needed for checking export paths
)
# Import core logic needed by fixtures/handlers (if handlers were tested directly)
from src.mcp_cadquery_server.core import execute_cqgi_script

# --- Fixtures ---

@pytest.fixture(scope="module")
def stored_build_result_id_for_handlers(): # Renamed to avoid conflict if run together
    """Creates a BuildResult and returns its ID for handler tests."""
    # Use core function directly
    script = "import cadquery as cq\nresult = cq.Workplane('XY').box(1, 1, 1)\nshow_object(result)" # Use show_object
    build_res = execute_cqgi_script(script)
    result_id = f"handler-test-{uuid.uuid4()}" # Make ID more specific
    shape_results[result_id] = build_res
    print(f"\nFixture: Created build result with ID {result_id}")
    return result_id

@pytest.fixture(autouse=True)
def manage_handler_state_and_files(stored_build_result_id_for_handlers):
    """Fixture to clear shape_results and render dir before/after each test."""
    # Clear state before test
    shape_results.clear()

    # Re-create the specific result needed for some tests, using the ID generated by the module fixture
    script = "import cadquery as cq\nresult = cq.Workplane('XY').box(1, 1, 1)\nshow_object(result)" # Use show_object
    build_res = execute_cqgi_script(script)
    if build_res.success and build_res.results: # Check results exist
        shape_results[stored_build_result_id_for_handlers] = build_res
        print(f"Auto-fixture: Re-created build result {stored_build_result_id_for_handlers}")
    else:
        pytest.fail("Failed to create the build result needed for handler tests in fixture.")

    # Clear render/preview directories (use imported paths)
    print("Auto-fixture: Clearing render/preview directories...")
    for dir_path in [RENDER_DIR_PATH, PART_PREVIEW_DIR_PATH]:
        if os.path.exists(dir_path):
            try: shutil.rmtree(dir_path)
            except OSError as e: print(f"Error removing directory {dir_path}: {e}")
        try: os.makedirs(dir_path, exist_ok=True)
        except OSError as e: pytest.fail(f"Failed to create directory {dir_path}: {e}")

    yield # Run the test

    # Clear state after test
    shape_results.clear()
    print("Auto-fixture: Cleared shape_results.")


# --- TestClient Fixture ---

@pytest.fixture(scope="module")
def client():
    """Provides a FastAPI TestClient instance using the global app."""
    # App is now globally defined after venv check
    with TestClient(app) as c:
        yield c

# --- Test Cases for /mcp/execute Endpoint ---
# These tests cover the functionality previously tested by direct handler calls

def test_mcp_execute_endpoint_script_success(client):
    """Test the /mcp/execute endpoint with a valid execute_cadquery_script request."""
    script = "import cadquery as cq\nresult = cq.Workplane('XY').sphere(5)\nshow_object(result)"
    request_id = f"test-endpoint-exec-{uuid.uuid4()}"
    request_body = {
        "request_id": request_id,
        "tool_name": "execute_cadquery_script",
        "arguments": {
            "script": script,
            "parameters": {}
        }
    }
    print(f"\nTesting POST /mcp/execute for execute_cadquery_script (ID: {request_id})...")
    response = client.post("/mcp/execute", json=request_body)
    assert response.status_code == 200
    assert response.json() == {"status": "processing", "request_id": request_id}

    # Allow some time for the background task to potentially run and populate state
    time.sleep(0.5) # Adjust delay as needed, not perfectly reliable
    result_id_expected = f"{request_id}_0" # Default convention
    assert result_id_expected in shape_results # Check if handler populated the result
    assert shape_results[result_id_expected].success is True
    assert len(shape_results[result_id_expected].results) == 1
    print("Side effect check (shape_results population) passed.")

    print("POST /mcp/execute for execute_cadquery_script test passed.")

def test_mcp_execute_endpoint_script_params_success(client):
    """Test the /mcp/execute endpoint with parameter substitution."""
    script = """
import cadquery as cq
length = 1.0 # PARAM
result = cq.Workplane("XY").box(length, 2, 1)
show_object(result)
"""
    request_id = f"test-endpoint-params-{uuid.uuid4()}"
    request_body = {
        "request_id": request_id,
        "tool_name": "execute_cadquery_script",
        "arguments": {
            "script": script,
            "parameter_sets": [{"length": 5.5}, {"length": 6.6}]
        }
    }
    print(f"\nTesting POST /mcp/execute with parameter_sets (ID: {request_id})...")
    response = client.post("/mcp/execute", json=request_body)
    assert response.status_code == 200
    assert response.json() == {"status": "processing", "request_id": request_id}

    # Allow time for background tasks
    time.sleep(0.5)
    result_id_0 = f"{request_id}_0"
    result_id_1 = f"{request_id}_1"
    assert result_id_0 in shape_results and result_id_1 in shape_results
    assert shape_results[result_id_0].success and shape_results[result_id_1].success
    # Basic check that results exist, dimension check is harder without direct access
    assert len(shape_results[result_id_0].results) == 1
    assert len(shape_results[result_id_1].results) == 1
    print("Side effect check (shape_results population for params) passed.")

    print("POST /mcp/execute with parameter_sets test passed.")


def test_mcp_execute_endpoint_export_svg_success(client, stored_build_result_id_for_handlers):
    """Test the /mcp/execute endpoint with a valid export_shape_to_svg request."""
    # Use the result_id created by the fixture
    result_id = stored_build_result_id_for_handlers
    print(f"Using result_id from fixture for SVG export test: {result_id}")

    request_id_export = f"test-endpoint-svg-{uuid.uuid4()}"
    custom_filename = f"test_render_{request_id_export}.svg"
    request_body = {
        "request_id": request_id_export,
        "tool_name": "export_shape_to_svg",
        "arguments": {
            "result_id": result_id, # Use the ID from the fixture
            "shape_index": 0,
            "filename": custom_filename
        }
    }
    print(f"\nTesting POST /mcp/execute for export_shape_to_svg (ID: {request_id_export})...")
    response = client.post("/mcp/execute", json=request_body)
    assert response.status_code == 200
    assert response.json() == {"status": "processing", "request_id": request_id_export}

    # Allow time for background task
    time.sleep(0.5)
    # Check side effect: Verify the file was created using imported path
    expected_path = os.path.join(RENDER_DIR_PATH, custom_filename)
    assert os.path.exists(expected_path)
    assert os.path.getsize(expected_path) > 0
    print(f"Side effect check (file creation: {expected_path}) passed.")

    print("POST /mcp/execute for export_shape_to_svg test passed.")


def test_mcp_execute_endpoint_missing_tool_name(client):
    """Test the /mcp/execute endpoint with missing tool_name."""
    request_id = f"test-endpoint-no-tool-{uuid.uuid4()}"
    request_body = {
        "request_id": request_id,
        # "tool_name": "missing",
        "arguments": {}
    }
    print(f"\nTesting POST /mcp/execute with missing tool_name (ID: {request_id})...")
    response = client.post("/mcp/execute", json=request_body)
    # The server should reject this immediately
    assert response.status_code == 400
    assert "Missing 'tool_name'" in response.text # Check error detail if possible
    print("POST /mcp/execute with missing tool_name test passed.")

def test_mcp_execute_endpoint_invalid_json(client):
    """Test the /mcp/execute endpoint with invalid JSON."""
    request_id = f"test-endpoint-bad-json-{uuid.uuid4()}"
    invalid_json_string = '{"request_id": "' + request_id + '", "tool_name": "test", "arguments": { "script": "..." ' # Intentionally broken JSON
    print(f"\nTesting POST /mcp/execute with invalid JSON (ID: {request_id})...")
    response = client.post("/mcp/execute", headers={"Content-Type": "application/json"}, content=invalid_json_string)
    assert response.status_code == 422 # Unprocessable Entity for invalid JSON body
    assert "detail" in response.json()
    print("POST /mcp/execute with invalid JSON test passed.")

# Tests for scan_part_library and search_parts via API can be added here
# They would require setting up files in the actual PART_LIBRARY_DIR
# and potentially waiting/polling for scan completion.